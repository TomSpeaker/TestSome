# é€šè¿‡CNNå®ç°ç®€å•çš„ç‰¹å®šäººè„¸è¯†åˆ«

## 0 ç¡¬ä»¶è¦æ±‚

### åŸºæœ¬è¦æ±‚

æœ€ååˆ¶ä½œçš„æ¨¡å‹å¤§å°å°±1Mbï¼ŒCPUå°±èƒ½è·‘ã€‚

ç”µè„‘å…·æœ‰æ‘„åƒå¤´

pythonç¯å¢ƒ

### éƒ¨ç½²ç¯å¢ƒ

æˆ‘ä½¿ç”¨çš„æ˜¯Python 3.8.18 (default, Sep 11 2023, 13:47:48) [MSC v.1916 64 bit (AMD64)] :: Anaconda,

```
#å®‰è£…ç¯å¢ƒåŒ…
pip install -r requirements.txt
```

### é¡¹ç›®æ–‡ä»¶ç»“æ„

```
ç‰¹å®šäººè„¸æ£€æµ‹
    â”‚  CreateNonFaceData.py
    â”‚  DetectManData.py
    â”‚  vedioback.mp4
    â”‚
    â”œâ”€background_cuts_from_video
    â”œâ”€CNNproject
    â”‚  â”‚  evalue.py
    â”‚  â”‚  face_cnn_model.pth
    â”‚  â”‚  model.py
    â”‚  â”‚  tainCNN.py
    â”‚  â”‚  TestByInput.py
    â”‚  â”‚
    â”‚  â”œâ”€dataset
    â”‚  â”‚  â”œâ”€face
    â”‚  â”‚  â””â”€nonface
    â”‚  â”œâ”€detected_faces
    â”‚  â””â”€__pycache__
    â”‚          evalue.cpython-38.pyc
    â”‚          model.cpython-38.pyc
    â”‚
    â”œâ”€CreateStandardImage
    â”‚      create.py
    â”‚
    â””â”€detected_faces
```

## 1 åˆ¶ä½œæ•°æ®é›†

### 1.1.1 åˆ¶ä½œè¦è¯†åˆ«çš„äººè„¸ä¿¡æ¯

CreateStandardImage\create.py 

**ç›®çš„ï¼š**æ”¶é›†æœ‰æ•ˆçš„å¯ä»¥è¢«æ­£ç¡®æ”¶é›†çš„äººè„¸ä¿¡æ¯

1. **æ‰“å¼€ç”µè„‘æ‘„åƒå¤´**ï¼Œè·å–å®æ—¶è§†é¢‘ç”»é¢ã€‚
2. **åœ¨ç”»é¢ä¸­å¤®ç»˜åˆ¶ä¸€ä¸ªçº¢è‰²çŸ©å½¢æ¡†**ï¼ˆ400Ã—400 åƒç´ ï¼Œç”¨äºå‚è€ƒåŒºåŸŸï¼‰ã€‚
3. ä½¿ç”¨ OpenCV çš„ **Haar ç‰¹å¾åˆ†ç±»å™¨æ£€æµ‹äººè„¸**ï¼Œåœ¨æ£€æµ‹åˆ°çš„äººè„¸å‘¨å›´ç»˜åˆ¶ **ç»¿è‰²çŸ©å½¢æ¡†**ã€‚
4. åœ¨å·¦ä¸Šè§’å®æ—¶æ˜¾ç¤º **å‰©ä½™å½•åˆ¶æ—¶é—´ï¼ˆç§’ï¼‰**ã€‚
5. å°† **åŸå§‹è§†é¢‘å¸§ï¼ˆä¸å¸¦ä»»ä½•çŸ©å½¢æ¡†ï¼‰ä¿å­˜ä¸º MP4 æ–‡ä»¶ `output.mp4`**ã€‚
6. å½“å½•åˆ¶æ—¶é—´è¾¾åˆ°è®¾å®šçš„ `record_time`ï¼ˆé»˜è®¤ 60ç§’ï¼‰æˆ–ç”¨æˆ·æŒ‰ä¸‹ `q` é”®æ—¶ï¼Œç¨‹åºç»“æŸã€‚

```python
import cv2
import time

# åŠ è½½ OpenCV äººè„¸æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# æ‰“å¼€é»˜è®¤æ‘„åƒå¤´
cap = cv2.VideoCapture(0)

# è·å–æ‘„åƒå¤´å›¾åƒçš„å®½é«˜
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))

# è®¾ç½®è¾“å‡ºè§†é¢‘å‚æ•°ï¼ˆä¸å¸¦çº¢æ¡†ï¼ŒMP4æ ¼å¼ï¼‰
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # æ”¹ä¸º 'mp4v' è¾“å‡º MP4 æ ¼å¼
out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (frame_width, frame_height))

# å®šä¹‰çº¢è‰²çŸ©å½¢æ¡†å¤§å°ï¼ˆä»¥ç”»é¢ä¸­å¿ƒä¸ºä¸­å¿ƒï¼‰
box_width, box_height = 400, 400

# è®¾ç½®å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰
record_time = 60  # å½•åˆ¶ç§’

# è·å–å½“å‰æ—¶é—´ï¼ˆå¼€å§‹æ—¶é—´ï¼‰
start_time = time.time()

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # è·å–å½“å‰æ—¶é—´å¹¶è®¡ç®—å·²å½•åˆ¶æ—¶é•¿
    elapsed_time = time.time() - start_time
    remaining_time = int(record_time - elapsed_time)  # è®¡ç®—å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰
    
    if remaining_time <= 0:
        break  # è¶…è¿‡å½•åˆ¶æ—¶é•¿åé€€å‡º

    # è½¬ä¸ºç°åº¦å›¾åƒç”¨äºäººè„¸æ£€æµ‹
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # ä½¿ç”¨äººè„¸æ£€æµ‹å™¨æ£€æµ‹äººè„¸
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)

    # ä¿å­˜ä¸å¸¦æ¡†çš„è§†é¢‘
    out.write(frame.copy())

    # ===== æ˜¾ç¤ºç”¨å¸§ï¼ˆåŠ çº¢æ¡†å’Œäººè„¸æ£€æµ‹æ¡†ï¼‰ =====
    display_frame = frame.copy()

    # ç”»çº¢è‰²çŸ©å½¢æ¡†ï¼ˆå±…ä¸­ï¼‰
    center_x, center_y = frame_width // 2, frame_height // 2
    x1 = center_x - box_width // 2
    y1 = center_y - box_height // 2
    x2 = center_x + box_width // 2
    y2 = center_y + box_height // 2
    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # çº¢è‰²çŸ©å½¢æ¡†

    # åœ¨æ¯ä¸€å¼ æ£€æµ‹åˆ°äººè„¸çš„åŒºåŸŸç”»ç»¿è‰²çŸ©å½¢æ¡†
    for (x, y, w, h) in faces:
        cv2.rectangle(display_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # ç»¿è‰²çŸ©å½¢æ¡†

    # æ˜¾ç¤ºå‰©ä½™æ—¶é—´
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.putText(display_frame, f'Remaining: {remaining_time}s', (10, 30), font, 1, (0, 255, 0), 2)

    # æ˜¾ç¤ºè§†é¢‘çª—å£
    cv2.imshow('Camera with Red Box and Face Detection', display_frame)

    # æŒ‰ q é€€å‡º
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é‡Šæ”¾èµ„æº
cap.release()
out.release()
cv2.destroyAllWindows()

print(f"å½•åˆ¶å®Œæˆï¼Œå·²ä¿å­˜ä¸º output.mp4")
```

**æ•ˆæœ**

![image-20250420100756742](images/1.png)

åˆ¶ä½œå®Œæ¯•åå¾—åˆ°output.mp4ï¼Œæ ¹æ®ç»¿è‰²æ¡†æ¡†å‡ºç°çš„æƒ…å†µï¼Œå¯ä»¥å¤§ä½“çŸ¥é“æˆ‘ä»¬èƒ½æ”¶é›†åˆ°å¤šå°‘äººè„¸æœ‰æ•ˆçš„ä¿¡æ¯ã€‚

### 1.1.2 å°†è§†é¢‘è½¬æ¢ä¸ºé€‚åˆç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚

DetectManData.py

**ç›®çš„**ï¼šå°†è§†é¢‘ä¸­çš„äººè„¸ä¿¡æ¯è½¬æ¢ä¸ºé€‚åˆç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚

**åŠ è½½äººè„¸æ£€æµ‹å™¨**ï¼šä½¿ç”¨ OpenCV è‡ªå¸¦çš„ `haarcascade_frontalface_default.xml` æ¨¡å‹æ¥æ£€æµ‹è§†é¢‘ä¸­çš„äººè„¸ã€‚

**è¯»å–è§†é¢‘**ï¼šé€šè¿‡ `cv2.VideoCapture` æ‰“å¼€å¹¶è¯»å–è§†é¢‘æ–‡ä»¶ã€‚

**åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹**ï¼šå¦‚æœæŒ‡å®šçš„æ–‡ä»¶å¤¹ï¼ˆ`detected_faces`ï¼‰ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºã€‚

**ä»è§†é¢‘å¸§ä¸­æ£€æµ‹äººè„¸**ï¼šé€å¸§å¤„ç†è§†é¢‘ï¼Œè½¬æ¢ä¸ºç°åº¦å›¾åƒï¼Œå¹¶é€šè¿‡ `detectMultiScale` æ–¹æ³•æ£€æµ‹å…¶ä¸­çš„äººè„¸ã€‚

**ç­›é€‰å’Œä¿å­˜äººè„¸**ï¼šå¯¹äºæ¯ä¸€å¸§ä¸­æ£€æµ‹åˆ°çš„äººè„¸ï¼Œå¦‚æœå®ƒä»¬çš„å®½é«˜å¤§äº **200x200**ï¼ˆç›®çš„å°±æ˜¯ç­›é€‰å‡ºäººè„¸ï¼‰ï¼Œå°±å°†å…¶æå–å‡ºæ¥å¹¶è°ƒæ•´ä¸º **50x50** çš„å°ºå¯¸ï¼Œç„¶åä¿å­˜ä¸º JPEG æ–‡ä»¶ã€‚ä¿å­˜çš„æ–‡ä»¶åä» `face_20.jpg` å¼€å§‹ï¼ˆç”± `start_index` æ§åˆ¶ï¼‰ï¼Œç›´åˆ°ä¿å­˜åˆ°æŒ‡å®šæ•°é‡ï¼ˆ`target_face_count`ï¼‰çš„å›¾åƒã€‚

**é€€å‡ºæ¡ä»¶**ï¼šå½“ä¿å­˜çš„äººè„¸æ•°é‡è¾¾åˆ°é¢„å®šçš„ç›®æ ‡æ•°é‡åï¼Œè„šæœ¬åœæ­¢å¤„ç†å¹¶é€€å‡ºã€‚

æ”¶é›†300ä¸ªæ•°æ®ã€‚

```python
import cv2
import os

# åŠ è½½ OpenCV äººè„¸æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
video_path = 'output.mp4'  # ä½ çš„è§†é¢‘è·¯å¾„
cap = cv2.VideoCapture(video_path)

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
output_folder = 'detected_faces'
os.makedirs(output_folder, exist_ok=True)

# è®¾ç½®èµ·å§‹ç¼–å·å’Œç›®æ ‡ä¿å­˜æ•°é‡
start_index = 0  # âœ… ä»ç¬¬å‡ å¼ å›¾ç‰‡å¼€å§‹å‘½å
target_face_count = 300  # âœ… æƒ³ä¿å­˜å¤šå°‘å¼ äººè„¸

frame_count = 0
saved_count = 0

while cap.isOpened() and saved_count < target_face_count:
    ret, frame = cap.read()
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # æ£€æµ‹äººè„¸
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))

    for i, (fx, fy, fw, fh) in enumerate(faces):
        if fw >200 and fh > 200:
            face_img = frame[fy:fy+fh, fx:fx+fw]
            fixed_size_face = cv2.resize(face_img, (50, 50))

            # ä½¿ç”¨ start_index ç”Ÿæˆæ–‡ä»¶å
            face_filename = os.path.join(output_folder, f"face_{start_index + saved_count}.jpg")
            cv2.imwrite(face_filename, fixed_size_face)
            saved_count += 1

            # è¾¾åˆ°ç›®æ ‡æ•°é‡åé€€å‡º
            if saved_count >= target_face_count:
                break

    frame_count += 1

cap.release()
cv2.destroyAllWindows()

print(f"å…±ä¿å­˜äº† {saved_count} å¼ äººè„¸å›¾åƒï¼Œä¿å­˜è‡³æ–‡ä»¶å¤¹ï¼š{output_folder}ï¼Œç¼–å·ä» face_{start_index}.jpg å¼€å§‹")
```

### 1.2 **åˆ¶ä½œç”¨äºéç‰¹å®šäººè„¸çš„æ•°æ®é›†**

CreateNonFaceData.py

**ç›®çš„**ï¼šåˆ¶ä½œç”¨äºéç‰¹å®šäººè„¸çš„æ•°æ®é›†

1. è¾“å…¥å¤„ç†ï¼š
   - è¯»å–åä¸º"vedioback.mp4"çš„è§†é¢‘æ–‡ä»¶
   - åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"background_cuts_from_video"
2. è£å‰ªé€»è¾‘ï¼š
   - ä»è§†é¢‘çš„æ¯ä¸€å¸§ä¸­ï¼Œä»¥50åƒç´ ä¸ºæ­¥é•¿ï¼ˆæ°´å¹³å’Œå‚ç›´æ–¹å‘ï¼‰æå–50x50åƒç´ çš„å°å›¾åƒå—
   - ç¡®ä¿ä¸ä¼šè¶…å‡ºå›¾åƒè¾¹ç•Œ
3. è¾“å‡ºæ§åˆ¶ï¼š
   - ä»æŒ‡å®šçš„èµ·å§‹ç¼–å·å¼€å§‹ä¿å­˜ï¼ˆé»˜è®¤ä»cut_0.jpgå¼€å§‹ï¼‰
   - æœ€å¤šä¿å­˜300å¼ å›¾åƒï¼ˆå¯è°ƒæ•´target_countï¼‰
   - å›¾åƒå‘½åä¸ºcut_XXX.jpgæ ¼å¼ï¼Œä¿å­˜åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸­
4. ç»ˆæ­¢æ¡ä»¶ï¼š
   - è¾¾åˆ°ç›®æ ‡æ•°é‡(300å¼ )æˆ–è§†é¢‘ç»“æŸå³åœæ­¢å¤„ç†

æœ€ç»ˆè¾“å‡ºï¼šåœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸­ç”Ÿæˆä¸€ç³»åˆ—50x50åƒç´ çš„å°å›¾åƒï¼Œç”¨äºåç»­å¤„ç†æˆ–ä½œä¸ºèƒŒæ™¯ç´ æã€‚

```python
import cv2
import os

# è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
video_path = 'vedioback.mp4'  # æ›¿æ¢ä¸ºä½ çš„è§†é¢‘è·¯å¾„
cap = cv2.VideoCapture(video_path)

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
output_folder = 'background_cuts_from_video'
os.makedirs(output_folder, exist_ok=True)

# âœ… è®¾ç½®èµ·å§‹ç¼–å·å’Œç›®æ ‡æ•°é‡
start_index = 0  # ä» cut_501.jpg å¼€å§‹ç¼–å·
target_count = 300  # è¦ä¿å­˜çš„å›¾åƒæ€»æ•°

frame_count = 0
cut_count = 0

while cap.isOpened() and cut_count < target_count:
    ret, frame = cap.read()
    if not ret:
        break

    height, width, _ = frame.shape

    for y in range(0, height, 50):
        for x in range(0, width, 50):
            if x + 50 <= width and y + 50 <= height:
                cut_img = frame[y:y+50, x:x+50]

                # ä¿å­˜å›¾åƒï¼Œç¼–å·ä» start_index å¼€å§‹
                cut_filename = os.path.join(output_folder, f"cut_{start_index + cut_count}.jpg")
                cv2.imwrite(cut_filename, cut_img)
                cut_count += 1

                if cut_count >= target_count:
                    break
        if cut_count >= target_count:
            break

    frame_count += 1

cap.release()
cv2.destroyAllWindows()

print(f"è£å‰ªå®Œæˆï¼Œå…±ç”Ÿæˆ {cut_count} å¼ å›¾åƒï¼Œç¼–å·ä» cut_{start_index}.jpg å¼€å§‹ï¼Œä¿å­˜è‡³æ–‡ä»¶å¤¹ï¼š{output_folder}")
```

## 2 åˆ¶ä½œæ¨¡å‹

### 2.1 è®¾è®¡æ¨¡å‹

CNNproject\model.py

**ç›®çš„**ï¼šæœ‰æ¨¡å‹æ‰èƒ½è®­ç»ƒ

ä¸æ‡‚çš„å°±å»æ‰¾ç›¸å…³èµ„æ–™ï¼Œå­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œ

```python
# ç®€å• CNN æ¨¡å‹
import torch
import torch.nn as nn
class FaceCNN(nn.Module):
    def __init__(self):
        super(FaceCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 25x25
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 12x12
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 6x6
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 6 * 6, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # è¾“å‡ºä¸¤ä¸ªç±»
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)
        return x

#mo = FaceCNN()

# from torchinfo import summary
# summary(mo, input_size=(1, 1, 50, 50))  # æ³¨æ„ batch_size=1
```

### 2.2  æŠŠä¹‹å‰åˆ¶ä½œçš„æ•°æ®é›†æ”¾å…¥å¯¹äºæ–‡ä»¶å¤¹

è¦ç‰¹å®šè¯†åˆ«çš„æ•°æ®é›†ï¼Œå°±æ”¾å…¥CNNproject\dataset\face

éç‰¹å®šè¯†åˆ«çš„æ•°æ®é›†ï¼Œå°±æ”¾å…¥CNNproject\dataset\nonfaceã€æ³¨æ„ï¼Œé™¤äº†ç‰¹å®šäººè„¸ï¼Œå…¶ä»–äººçš„äººè„¸çš„æ•°æ®é›†ï¼ˆä¹Ÿèƒ½é€šè¿‡1.1æ”¶é›†ï¼‰ä¹Ÿèƒ½æ”¾å…¥è¿™ä¸ªæ•°æ®é›†ã€‘

## 3 è®­ç»ƒæ¨¡å‹

tainCNN.py

**ç›®çš„**ï¼šè®­ç»ƒæ¨¡å‹ï¼Œåˆ¶ä½œå‡ºå¯ä»¥è¯†åˆ«ç‰¹å®šäººè„¸çš„æ¨¡å‹

1. **æ•°æ®é¢„å¤„ç†**
   - å°†è¾“å…¥å›¾åƒç»Ÿä¸€è½¬ä¸º **ç°åº¦å›¾**ï¼ˆ`Grayscale`ï¼‰ã€‚
   - è°ƒæ•´å°ºå¯¸ä¸º **50Ã—50**ï¼ˆ`Resize`ï¼‰ã€‚
   - è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼ˆ`ToTensor`ï¼‰ã€‚
2. **æ•°æ®é›†åŠ è½½**
   - ä» `dataset_path` æ–‡ä»¶å¤¹è¯»å–æ•°æ®ï¼Œè‡ªåŠ¨æ ¹æ®å­æ–‡ä»¶å¤¹åç”Ÿæˆç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚ `["class0", "class1"]`ï¼‰ã€‚
   - ä½¿ç”¨ `DataLoader` åˆ†æ‰¹æ¬¡åŠ è½½æ•°æ®ï¼ˆ`batch_size=32`ï¼‰ï¼Œæ”¯æŒéšæœºæ‰“ä¹±ï¼ˆ`shuffle=True`ï¼‰ã€‚
3. **æ¨¡å‹è®­ç»ƒ**
   - ä½¿ç”¨è‡ªå®šä¹‰çš„ `FaceCNN` æ¨¡å‹ï¼ˆä¸€ä¸ªç®€å•çš„3å±‚CNNï¼Œè¾“å‡º2åˆ†ç±»ç»“æœï¼‰ã€‚
   - æ”¯æŒ **ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ**ï¼ˆ`load_model=True`æ—¶åŠ è½½ `face_cnn_model.pth`ï¼‰ã€‚
   - ä¼˜åŒ–å™¨ä¸º **Adam**ï¼ŒæŸå¤±å‡½æ•°ä¸º **äº¤å‰ç†µæŸå¤±**ï¼ˆ`CrossEntropyLoss`ï¼‰ã€‚
   - æ¯è½®ï¼ˆepochï¼‰ç»Ÿè®¡ **æ€»æŸå¤±** å’Œ **åˆ†ç±»å‡†ç¡®ç‡**ï¼Œå¹¶æ‰“å°æ—¥å¿—ã€‚
4. **æ¨¡å‹ä¿å­˜**
   - è®­ç»ƒå®Œæˆåï¼Œå°†æ¨¡å‹å‚æ•°ä¿å­˜åˆ° `face_cnn_model.pth`ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import os
from model import FaceCNN


def train_model(
    dataset_path='dataset',
    model_path='face_cnn_model.pth',
    load_model=False,
    batch_size=32,
    num_epochs=10,
    learning_rate=0.001,
    image_size=50
):
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor()
    ])

    # åŠ è½½æ•°æ®é›†
    dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    print(f"æ£€æµ‹åˆ°çš„ç±»åˆ«ï¼š{dataset.classes}")

    # æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = FaceCNN().to(device)
    if load_model and os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹å‚æ•°ï¼š{model_path}")
    else:
        print("ğŸš€ ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # è®­ç»ƒè¿‡ç¨‹
    for epoch in range(num_epochs):
        model.train()
        total_loss, correct, total = 0, 0, 0

        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        acc = correct / total
        print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss:.4f}, Accuracy: {acc:.4f}")

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{model_path}")

# ç¤ºä¾‹è°ƒç”¨ï¼š
if __name__ == '__main__':
    train_model(load_model=True,num_epochs=60)  # å¦‚æœè¦åŠ è½½å·²æœ‰æ¨¡å‹æ¥ç€è®­ç»ƒï¼Œæ”¹ä¸º load_model=True
```

**æ•ˆæœ**

![å±å¹•æˆªå›¾ 2025-04-20 110201](images/2.png)

## 4 æ£€éªŒæ¨¡å‹

evalue.py

ç›®çš„ï¼šå°è£…æ–¹æ³•ï¼Œæ›´ä¾¿æ·ä½¿ç”¨æ¨¡å‹åŠŸèƒ½

1. **æ¨¡å‹åŠ è½½**
   - è‡ªåŠ¨æ£€æµ‹GPU/CPUè®¾å¤‡ï¼ŒåŠ è½½é¢„è®­ç»ƒçš„`FaceCNN`æ¨¡å‹ï¼ˆä½ ä¹‹å‰å®šä¹‰çš„50Ã—50ç°åº¦å›¾CNNï¼‰ã€‚
   - ä»`model_path`åŠ è½½è®­ç»ƒå¥½çš„æƒé‡å‚æ•°ï¼Œå¹¶åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆ`model.eval()`ï¼‰ã€‚
2. **å›¾åƒé¢„å¤„ç†**
   - å°†è¾“å…¥å›¾åƒè½¬ä¸ºç°åº¦å›¾ï¼Œç»Ÿä¸€ç¼©æ”¾åˆ°50Ã—50å°ºå¯¸ï¼Œè½¬æ¢ä¸ºPyTorchå¼ é‡ã€‚
   - æ·»åŠ batchç»´åº¦ï¼ˆ`unsqueeze(0)`ï¼‰é€‚é…æ¨¡å‹è¾“å…¥è¦æ±‚ `[1, 1, 50, 50]`ã€‚
3. **æ¨ç†é¢„æµ‹**
   - ä½¿ç”¨`torch.no_grad()`ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œæå‡æ¨ç†é€Ÿåº¦ã€‚
   - æ¨¡å‹è¾“å‡ºé€šè¿‡`softmax`è½¬ä¸ºæ¦‚ç‡å€¼ï¼Œå–ç´¢å¼•0çš„æ¦‚ç‡ä½œä¸º"ä½ çš„è„¸"çš„ç½®ä¿¡åº¦ã€‚
   - æ ¹æ®é˜ˆå€¼ï¼ˆé»˜è®¤0.9ï¼‰åˆ¤æ–­åˆ†ç±»ç»“æœï¼š
     - æ¦‚ç‡â‰¥é˜ˆå€¼ â†’ åˆ¤å®šä¸º"ä½ çš„è„¸"ï¼ˆè¿”å›1ï¼‰
     - å¦åˆ™ â†’ åˆ¤å®šä¸º"ä¸æ˜¯ä½ çš„è„¸"ï¼ˆè¿”å›0ï¼‰
4. **ç»“æœè¾“å‡º**
   - æ‰“å°é¢„æµ‹æ¦‚ç‡å’Œåˆ†ç±»ç»“æœï¼ˆå¦‚ï¼š`å›¾ç‰‡face_214.jpg çš„é¢„æµ‹æ¦‚ç‡ä¸º0.1234ï¼Œåˆ¤æ–­ä¸ºï¼šä¸æ˜¯ä½ çš„è„¸`ï¼‰ã€‚

```python
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
from model import FaceCNN  # ä½ ä¹‹å‰å®šä¹‰çš„ CNN æ¨¡å‹

def predict_image(image_path, model_path='face_cnn_model.pth', image_size=50, threshold=0.9):
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor()
    ])

    # åŠ è½½å›¾åƒ
    image = Image.open(image_path)
    image = transform(image).unsqueeze(0)  # åŠ ä¸Š batch ç»´åº¦ [1, 1, 50, 50]

    # åŠ è½½æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = FaceCNN().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    # æ¨ç†
    with torch.no_grad():
        image = image.to(device)
        output = model(image)
        probs = torch.softmax(output, dim=1)
        face_prob = probs[0][0].item()  # å‡è®¾ç´¢å¼•0æ˜¯â€œä½ çš„è„¸â€

    # åˆ¤æ–­æ˜¯å¦æ˜¯â€œä½ çš„è„¸â€
    result = 1 if face_prob >= threshold else 0
    print(f"å›¾ç‰‡ {image_path} çš„é¢„æµ‹æ¦‚ç‡ä¸º {face_prob:.4f}ï¼Œåˆ¤æ–­ä¸ºï¼š{'ä½ çš„è„¸' if result == 1 else 'ä¸æ˜¯ä½ çš„è„¸'}")
    return result


# ç¤ºä¾‹è°ƒç”¨
if __name__ == '__main__':
    test_image = r'C:\Users\26423\Desktop\pythonTest\AiTest\äººä½“æ£€æµ‹\CNNproject\dataset\nonface\face_214.jpg'  # æ›¿æ¢ä¸ºä½ çš„æµ‹è¯•å›¾ç‰‡è·¯å¾„
    print(predict_image(test_image,threshold=0.95))
```

æ›´æ–¹ä¾¿çš„éªŒè¯

TestByInput.py

```python
from evalue import predict_image

while True:
    test_image = input("å›¾ç‰‡è·¯å¾„:")
    print(predict_image(test_image,threshold=0.85))
```

**æ•ˆæœ**

![å±å¹•æˆªå›¾ 2025-04-20 111515](images/3.png)

## 5 åŠŸèƒ½ç”¨é€”

### **ğŸ› ï¸ åº”ç”¨åœºæ™¯**

1. **é—¨ç¦/è€ƒå‹¤ç³»ç»Ÿ**
   - è¯†åˆ«ç‰¹å®šäººå‘˜ï¼ˆå¦‚å…¬å¸å‘˜å·¥ã€å®¶åº­æˆå‘˜ï¼‰ã€‚
2. **è‡ªåŠ¨åŒ–è®¾å¤‡æ§åˆ¶**
   - ä»…å¯¹ç›®æ ‡ç”¨æˆ·å“åº”ï¼ˆå¦‚ä¸ªäººç”µè„‘è§£é”ã€æ™ºèƒ½å®¶å±…æ¿€æ´»ï¼‰ã€‚
3. **å†…å®¹è¿‡æ»¤**
   - å±è”½éç›®æ ‡äººè„¸çš„ç…§ç‰‡/è§†é¢‘ï¼ˆå¦‚å„¿ç«¥ç›¸å†Œåªä¿ç•™çˆ¶æ¯å‡ºé•œç‰‡æ®µï¼‰ã€‚
4. **æ•™è‚²æ¼”ç¤º**
   - å­¦ä¹ PyTorchæ¨¡å‹è®­ç»ƒå’ŒOpenCVå›¾åƒå¤„ç†çš„å®Œæ•´æµç¨‹ã€‚

### **âš¡ ä¼˜åŠ¿ç‰¹ç‚¹**

- **è½»é‡åŒ–**ï¼šæ¨¡å‹<1MBï¼Œé€‚åˆåµŒå…¥å¼è®¾å¤‡ã€‚
- **ä½ä¾èµ–**ï¼šä»…éœ€OpenCV+PyTorchåŸºç¡€ç¯å¢ƒã€‚
- **å¯å®šåˆ¶**ï¼šé€šè¿‡è°ƒæ•´æ•°æ®é›†å’Œé˜ˆå€¼é€‚é…ä¸åŒäººè„¸ã€‚
- **è‡ªåŠ¨åŒ–**ï¼šä»æ•°æ®é‡‡é›†åˆ°è®­ç»ƒå…¨æµç¨‹è„šæœ¬åŒ–ã€‚

------

### **ğŸ“Œ æ³¨æ„äº‹é¡¹**

1. **å…‰ç…§/è§’åº¦è¦æ±‚**
   - è®­ç»ƒæ•°æ®åº”è¦†ç›–å¤šç§å…‰ç…§å’Œå§¿æ€ï¼ˆå¦‚ä¾§è„¸ã€æˆ´çœ¼é•œç­‰ï¼‰ã€‚
2. **èƒŒæ™¯å¤æ‚åº¦**
   - ç®€å•èƒŒæ™¯ï¼ˆå¦‚ç™½å¢™ï¼‰ä¸‹è¯†åˆ«æ•ˆæœæ›´ä½³ã€‚
3. **é˜ˆå€¼è°ƒä¼˜**
   - è¿‡é«˜å¯èƒ½å¯¼è‡´æ¼æ£€ï¼Œè¿‡ä½å¯èƒ½è¯¯æ£€ï¼Œéœ€æ ¹æ®å®é™…æµ‹è¯•è°ƒæ•´ã€‚

------

### **ğŸš€ æ‰©å±•æ–¹å‘**

- **å¢åŠ æ•°æ®å¢å¼º**ï¼šè®­ç»ƒæ—¶åŠ å…¥æ—‹è½¬ã€ç¿»è½¬æå‡é²æ£’æ€§ã€‚
- **å®æ—¶æ‘„åƒå¤´æ£€æµ‹**ï¼šç»“åˆOpenCVå®ç°è§†é¢‘æµå®æ—¶è¯†åˆ«ã€‚
- **å¤šç±»åˆ«æ”¯æŒ**ï¼šä¿®æ”¹æ¨¡å‹è¾“å‡ºå±‚ï¼Œæ”¯æŒè¯†åˆ«å¤šä¸ªäººã€‚
